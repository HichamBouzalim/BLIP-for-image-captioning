# BLIP Image Captioning with Hugging Face Transformers

[![Python](https://img.shields.io/badge/Python-3.11+-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-BLIP-orange)](https://huggingface.co/Salesforce/blip-image-captioning-base)

**Estimated Reading Time:** 15 minutes

---

## ðŸš€ Overview
This repository demonstrates how to use **BLIP (Bootstrapping Language-Image Pretraining)** from Hugging Face Transformers for:
- Image captioning
- Visual question answering (VQA)

BLIP bridges the gap between images and text, enabling AI models to generate accurate captions, answer questions about images, and support content accessibility.

---

## ðŸ§° Features
- Generate descriptive captions for images automatically
- Answer questions about images
- Support multimodal AI applications combining vision and language

---

## âš¡ Installation

Install Python libraries:

```bash
pip install transformers Pillow torch torchvision torchaudio requests
